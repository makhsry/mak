{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPIqFbjXDsuPsSbpDU4s04C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Explanations, tradeoffs, and tips (detailed justification)\n","\n","Why Demucs (recommended)\n","\n","Demucs operates primarily in the time domain with learned architectures that model phase implicitly. That gives perceptually better results vs simple magnitude NMF where phase is re-used and causes artifacts. Demucs models (demucs / htdemucs) are trained on many source separation datasets — they generalize well to speech+background-music, music separations, and many real-world mixes. Use GPU.\n","\n","Why chunking & overlap-add\n","\n","Demucs runs on entire audio; very long files can exhaust GPU memory. Chunking into overlapping windows + overlap-add preserves the original sample rate and avoids artifacts at chunk boundaries when using a modest overlap (1 s is a good default). Overlap-add with normalization keeps amplitudes consistent.\n","\n","Avoiding downgrades\n","\n","We use ffmpeg to convert to 32-bit float WAV at the original sample rate (no resampling). Processing in float prevents quantization/rounding. When exporting we use 32-bit float WAV (subtype='FLOAT') so there is no loss. Only when you or a target system demands 16-bit should you convert back.\n","\n","Alternative methods\n","\n","Spleeter is fast and CPU-friendly but uses spectrogram-based NNs and often performs worse than Demucs on complex mixes. Useful if you need quick stems (2/4/5 stems).\n","\n","Open-Unmix (UMX) and Conv-TasNet are other high-quality options for certain tasks. Demucs is a general strong first choice.\n","\n","Model selection\n","\n","Try multiple Demucs variants if you want: demucs, htdemucs, htdemucs_ft (fine-tuned), etc. The pretrained.get_model() call will download available weights. The larger the model, the better but the more GPU/VRAM needed.\n","\n","Quality vs runtime\n","\n","If you need best quality, increase CHUNK_SECONDS (so model sees larger context) and use the largest Demucs variant. That increases GPU memory and runtime.\n","\n","For faster results, use smaller model or Spleeter; but expect more bleed/artifacts.\n","\n","Labeling & post-filtering\n","\n","Automatic labeling heuristics are approximate. For production, listen to the stems; then optionally run targeted denoisers (e.g., speech enhancement) on stems you identify as speech to further reduce residual background.\n","\n","Verifying results\n","\n","I included waveform overlays and spectrograms so you can visually check separation quality quickly. Listen to the WAV outputs as final verification."],"metadata":{"id":"_TpYJPoyin7X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDs_UFyFf9Ac"},"outputs":[],"source":["# -----------------------------------------------------------------------------\n","# 0. Settings (edit these)\n","# -----------------------------------------------------------------------------\n","INPUT_FILE = \"/content/60s-20.m4a\"   # path to your uploaded file in Colab\n","OUTPUT_DIR = \"/content/separated_outputs\"  # where separated stems and plots will be saved\n","USE_DEMUCS = True   # True -> Demucs (recommended). If False, uses Spleeter (fast) fallback.\n","DEMUC_MODEL = \"htdemucs\"  # recommended: \"demucs\" or \"htdemucs\" (both available). See notes below.\n","CHUNK_SECONDS = 6   # chunk size for long files (10s is safe; increase if you have GPU/RAM) - Reduced to 6 seconds\n","CHUNK_OVERLAP = 1.0  # overlap in seconds between chunks to avoid boundary artifacts\n","PRESERVE_SR = True   # preserve original sample rate (recommended)\n","# -----------------------------------------------------------------------------"]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 1. Install dependencies (run in Colab cell)\n","# Explanation:\n","#  - We install demucs for high-quality separation (uses PyTorch + GPU).\n","#  - We also install spleeter and librosa for fallback and plotting/analysis.\n","#  - ffmpeg for format conversion and so we always work with a WAV copy at exact sample rate.\n","# -----------------------------------------------------------------------------\n","!pip install -q demucs  # high-quality separator (recommended)\n","!pip install -q spleeter  # alternative (fast, but lower quality for some sources)\n","!pip install -q librosa soundfile matplotlib numpy scipy\n","!apt-get -qq update && apt-get -qq install -y ffmpeg"],"metadata":{"id":"DaZyL228iOCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PVuT8xRDj1Dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# 0. Clean + Install dependencies for Demucs separation (Colab-safe)\n","# ============================================================================\n","!pip install --upgrade pip setuptools wheel\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # CUDA 12.1 build (for GPUs)\n","!pip install demucs==4.0.0 --no-build-isolation\n","!pip install spleeter librosa soundfile matplotlib numpy scipy tqdm ffmpeg-python\n","!apt-get -qq update && apt-get -qq install -y ffmpeg\n"],"metadata":{"id":"vOekzMT-j1A9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 1. Configuration\n","# -----------------------------------------------------------------------------\n","INPUT_FILE = \"/content/60s-20.m4a\"\n","OUTPUT_DIR = \"/content/separated_outputs\"\n","USE_DEMUCS = True\n","DEMUC_MODEL = \"htdemucs\"\n","CHUNK_SECONDS = 6\n","CHUNK_OVERLAP = 1.0\n","PRESERVE_SR = True\n"],"metadata":{"id":"Yj7bz82Dj0-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qyki9QDsj07V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Dkr3nftMj01u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5MAhS5N1j0yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 2. Utilities: safe WAV conversion and helpers\n","# Explanation:\n","#  - Always work from a WAV copy converted by ffmpeg at the original sample rate to avoid\n","#    hidden codec/resampling issues.\n","#  - We keep the original sample rate and bit depth (ffmpeg parameter).\n","# -----------------------------------------------------------------------------\n","import os, subprocess, math, shutil\n","from pathlib import Path\n","import soundfile as sf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","def to_wav_preserve(input_path, out_wav=None):\n","    \"\"\"\n","    Convert input audio to 32-bit float WAV preserving sample rate.\n","    Using 32-bit float avoids clipping during processing and keeps fidelity.\n","    \"\"\"\n","    if out_wav is None:\n","        stem = Path(input_path).stem\n","        out_wav = f\"/content/{stem}_converted.wav\"\n","    # ffmpeg: -vn disable video, -y overwrite; -map 0 selects all audio streams\n","    # Use -c:a pcm_f32le to output 32-bit float WAV (preserves high fidelity)\n","    cmd = [\"ffmpeg\", \"-y\", \"-i\", str(input_path), \"-vn\", \"-map\", \"0:a:0\", \"-c:a\", \"pcm_f32le\", str(out_wav)]\n","    print(\"Running ffmpeg to convert to WAV (32-bit float):\", \" \".join(cmd))\n","    subprocess.run(cmd, check=True)\n","    return out_wav\n","\n","# convert input to wav (preserve original's sample rate inside a float WAV)\n","wav_path = to_wav_preserve(INPUT_FILE)\n","print(\"Converted WAV path:\", wav_path)\n","\n","# read metadata\n","data, sr = sf.read(wav_path, dtype='float32')\n","print(f\"Loaded converted WAV: duration={len(data)/sr:.2f}s, sr={sr}, samples={len(data)}\")\n"],"metadata":{"id":"ROIkPXEPiQyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 3a. HIGH QUALITY Separation with Demucs (recommended)\n","# Explanation / justification:\n","#  - Demucs (Facebook research) uses time-domain models and often yields best perceptual separation\n","#    for music and multi-source audio. It preserves sample rate and works in time domain so phase issues\n","#    are better handled than simple magnitude-based NMF.\n","#  - We process the file in chunks (CHUNK_SECONDS) with overlap (CHUNK_OVERLAP) to avoid OOM for long audio,\n","#    then overlap-add outputs. This keeps fidelity (no downsampling) and lets you process arbitrarily long files.\n","# -----------------------------------------------------------------------------\n","if USE_DEMUCS:\n","    # We call demucs via its Python API so we have programmatic control over models and chunking.\n","    import torch\n","    from demucs import pretrained\n","    from demucs.apply import apply_model\n","    from demucs.audio import AudioFile\n","\n","    # Select model - \"demucs\" or \"htdemucs\" are common names; pick based on your GPU:\n","    MODEL = DEMUC_MODEL\n","\n","    print(\"Loading Demucs model:\", MODEL)\n","    # get_model downloads pretrained weights the first time (colab internet required)\n","    model = pretrained.get_model(MODEL)\n","    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.eval()\n","    print(\"Model loaded on\", next(model.parameters()).device)\n","\n","    # chunking helpers\n","    def split_into_chunks(signal, sr, chunk_sec=CHUNK_SECONDS, overlap_sec=CHUNK_OVERLAP):\n","        step = int((chunk_sec - overlap_sec) * sr)\n","        win = int(chunk_sec * sr)\n","        if step <= 0:\n","            raise ValueError(\"chunk_sec must be > overlap_sec.\")\n","        chunks = []\n","        idx = 0\n","        while idx < len(signal):\n","            chunk = signal[idx: idx + win]\n","            chunks.append((idx, chunk))\n","            idx += step\n","        return chunks, win, step\n","\n","    def overlap_add_segments(segments, length, step):\n","        \"\"\"\n","        segments: list of numpy arrays (separated audio for a chunk)\n","        length: full length expected\n","        step: hop between segments\n","        We'll reconstruct by summing overlapped areas and normalizing by overlap count.\n","        \"\"\"\n","        # segments is list of arrays shape (n_sources, samples) for each chunk\n","        n_chunks = len(segments)\n","        # determine n_sources\n","        n_sources = segments[0].shape[0]\n","        out = [np.zeros(length, dtype=np.float32) for _ in range(n_sources)]\n","        counts = np.zeros(length, dtype=np.float32)\n","        pos = 0\n","        for seg in segments:\n","            seg_len = seg.shape[1]\n","            for s in range(n_sources):\n","                end = pos + seg_len\n","                out[s][pos:end] += seg[s]\n","            counts[pos:pos+seg_len] += 1.0\n","            pos += step\n","        # avoid division by zero\n","        counts[counts == 0] = 1.0\n","        for s in range(n_sources):\n","            out[s] /= counts\n","        return out\n","\n","    # prepare mono/stereo handling: demucs expects (channels, samples). We will feed original channels.\n","    # Use soundfile to read the original converted WAV with original channels.\n","    full_audio, full_sr = sf.read(wav_path, dtype='float32')  # could be mono or stereo\n","    if full_audio.ndim == 1:\n","        channels = 1\n","        full_audio = full_audio[np.newaxis, :]  # shape (1, samples)\n","    else:\n","        # transpose to (channels, samples) for demucs API\n","        full_audio = full_audio.T\n","\n","    print(\"Full audio shape (channels, samples):\", full_audio.shape)\n","\n","    chunks, win, step = split_into_chunks(full_audio[0], full_sr, CHUNK_SECONDS, CHUNK_OVERLAP)\n","    print(f\"Splitting into {len(chunks)} chunks; window={win} samples, step={step} samples\")\n","\n","    separated_chunks = []  # each element: numpy array shape (n_sources, samples)\n","    for i, (start, chunk0) in enumerate(chunks):\n","        # slice all channels for this chunk\n","        chunk_slice = full_audio[:, start:start+win]\n","        # If chunk shorter than window, pad with zeros (keeps exact sample-rate)\n","        if chunk_slice.shape[1] < win:\n","            pad_width = win - chunk_slice.shape[1]\n","            chunk_slice = np.pad(chunk_slice, ((0,0),(0,pad_width)), mode='constant')\n","\n","        # demucs apply_model expects a tensor shape (1, channels, samples) (mono/stereo handled)\n","        with torch.no_grad():\n","            wav_tensor = torch.from_numpy(chunk_slice).unsqueeze(0)  # (1, channels, samples)\n","            wav_tensor = wav_tensor.to(next(model.parameters()).device)\n","            # apply_model returns a dict with 'sources' array shape (batch, sources, channels, samples)\n","            est = apply_model(model, wav_tensor, device=next(model.parameters()).device, split=False, overlap=0)\n","            # move to CPU numpy\n","            est_np = est.cpu().numpy()[0]  # shape (sources, channels, samples)\n","            # If demucs returns (sources, channels, samples) and we want per-source mixed channels,\n","            # we can average channels or keep all channels; we'll mix channels to original channel count.\n","            # Here we will mix channels down to mono if original was mono, otherwise keep stereo by averaging across source channels.\n","            # For simplicity, sum across source channels to produce one signal per source (mono); to preserve stereo,\n","            # one could write separate stereo files by keeping the two channels.\n","            # We'll collapse source channels by averaging across the channel axis:\n","            est_mono = est_np.mean(axis=1)  # shape (sources, samples)\n","            separated_chunks.append(est_mono.astype(np.float32))\n","\n","        print(f\"Processed chunk {i+1}/{len(chunks)} (start={start})\")\n","\n","    # overlap-add to reconstruct full-length separated sources (mono per source)\n","    segments = separated_chunks\n","    full_length = full_audio.shape[1]\n","    reconstructed_sources = overlap_add_segments(segments, full_length, step)\n","\n","    # write each source to WAV (preserve sample rate, 32-bit float)\n","    out_paths = []\n","    for idx, src in enumerate(reconstructed_sources, start=1):\n","        outp = os.path.join(OUTPUT_DIR, f\"{Path(INPUT_FILE).stem}_demucs_component_{idx}.wav\")\n","        sf.write(outp, src, full_sr, subtype='FLOAT')\n","        out_paths.append(outp)\n","        print(\"Wrote:\", outp)\n","\n","    print(\"Demucs separation done. Outputs:\", out_paths)"],"metadata":{"id":"wvymSrGIiSxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 3b. Alternative: Spleeter (fast, but lower quality for some mixes)\n","# Explanation:\n","#  - Spleeter is faster and good for common music separations (2/4/5 stems).\n","#  - Use it if you need quick stems without GPU. It operates with fixed stem labels (vocals, drums, bass, other).\n","# -----------------------------------------------------------------------------\n","if not USE_DEMUCS:\n","    # Example: 4-stem separation (vocals, drums, bass, other)\n","    from spleeter.separator import Separator\n","    separator = Separator('spleeter:4stems')  # 2stems/4stems/5stems available\n","    out_dir = os.path.join(OUTPUT_DIR, \"spleeter_out\")\n","    os.makedirs(out_dir, exist_ok=True)\n","    separator.separate_to_file(wav_path, out_dir)\n","    print(\"Spleeter separation completed; check directory:\", out_dir)\n"],"metadata":{"id":"quFtUuAziYsL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 4. Visualize: overlayed waveforms and spectrogram of separated outputs\n","# Explanation:\n","#  - Plot normalized overlayed waveforms so you can visually compare sources without amplitude differences hiding detail.\n","#  - Also plot spectrograms for each source to inspect frequency/time content.\n","# -----------------------------------------------------------------------------\n","import librosa, librosa.display\n","import matplotlib.pyplot as plt\n","\n","# Gather output files from the chosen method:\n","if USE_DEMUCS:\n","    out_files = out_paths\n","else:\n","    # gather spleeter outputs\n","    out_files = [str(p) for p in Path(OUTPUT_DIR).glob(\"**/*.wav\")]\n","\n","# load mixture for overlay plot:\n","mixture, sr = sf.read(wav_path, dtype='float32')\n","if mixture.ndim > 1:\n","    mixture_mono = mixture.mean(axis=1)\n","else:\n","    mixture_mono = mixture\n","\n","# plot overlay\n","plt.figure(figsize=(14, 5))\n","t = np.arange(len(mixture_mono))/sr\n","plt.plot(t, mixture_mono / (np.max(np.abs(mixture_mono)) + 1e-9), alpha=0.25, linewidth=0.7, label=\"mixture (normalized)\")\n","for i, fpath in enumerate(out_files):\n","    sig, _ = sf.read(fpath, dtype='float32')\n","    if sig.ndim > 1:\n","        sig = sig.mean(axis=1)\n","    # align length\n","    sig = sig[:len(mixture_mono)]\n","    sign = sig / (np.max(np.abs(sig)) + 1e-9)\n","    plt.plot(t, sign + 0.0, linewidth=0.9, label=f\"component {i+1}\")\n","plt.xlim(0, len(mixture_mono)/sr)\n","plt.xlabel(\"Time (s)\")\n","plt.ylabel(\"Normalized amplitude\")\n","plt.legend(loc='upper right')\n","plt.title(\"Overlayed waveforms (normalized) — mixture + separated components\")\n","plt.show()\n","\n","# spectrograms\n","n = len(out_files)\n","cols = 2\n","rows = math.ceil((n+1)/cols)\n","plt.figure(figsize=(14, 3*rows))\n","# mixture spectrogram\n","plt.subplot(rows, cols, 1)\n","D = np.abs(librosa.stft(mixture_mono, n_fft=2048, hop_length=512))\n","librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n","plt.title(\"Mixture spectrogram\")\n","plt.colorbar(format=\"%+2.0f dB\")\n","\n","for i, fpath in enumerate(out_files, start=1):\n","    plt.subplot(rows, cols, i+1)\n","    sig, _ = sf.read(fpath, dtype='float32')\n","    if sig.ndim > 1:\n","        sig = sig.mean(axis=1)\n","    D = np.abs(librosa.stft(sig, n_fft=2048, hop_length=512))\n","    librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n","    plt.title(f\"Component {i} spectrogram\")\n","    plt.colorbar(format=\"%+2.0f dB\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"c_Ie02eYicKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 5. (Optional) Auto-label components by simple heuristics\n","# Explanation:\n","#  - Use spectral centroid and energy fraction to guess if a stem is voice / music / hum / noise.\n","#  - Heuristics are imperfect; consider manual review for critical labeling.\n","# -----------------------------------------------------------------------------\n","def guess_label(signal, sr):\n","    # compute spectral centroid and RMS energy\n","    import librosa\n","    if signal.ndim > 1:\n","        signal = signal.mean(axis=1)\n","    centroid = librosa.feature.spectral_centroid(y=signal, sr=sr).mean()\n","    rms = librosa.feature.rms(y=signal).mean()\n","    # low-frequency hum detection: centroid very low + narrow band energy\n","    if centroid < 200 and rms > 1e-5:\n","        return \"low-hum/low-frequency noise\"\n","    # speech heuristic: mid centroid and strong fluctuations (energy variance)\n","    if 200 < centroid < 3000 and rms < 0.02:\n","        return \"possible speech/voice\"\n","    # music: broader bandwidth and higher centroid\n","    if centroid >= 3000:\n","        return \"music/bright content\"\n","    return \"unknown\"\n","\n","for f in out_files:\n","    s, _ = sf.read(f, dtype='float32')\n","    if s.ndim > 1:\n","        s_m = s.mean(axis=1)\n","    else:\n","        s_m = s\n","    label = guess_label(s_m, sr)\n","    print(Path(f).name, \"->\", label)\n"],"metadata":{"id":"NmgVR8rXieKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# 6. Packaging outputs\n","# -----------------------------------------------------------------------------\n","# Create a zip with all separated stems and the plots (if you saved plots).\n","shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n","print(\"Created archive:\", OUTPUT_DIR + \".zip\")\n"],"metadata":{"id":"sVYhLwbAigGy"},"execution_count":null,"outputs":[]}]}