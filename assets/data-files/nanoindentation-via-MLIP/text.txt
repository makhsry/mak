
The ability of materials to resist against deformation, to which might be exposed through a wide variety of acting forces including indentation, scratching, bending, abrasion, cutting or penetration and etc., is referred as hardness (H) \cite{001}. Such definition suffers from the locality issues i.e. hardness would depend on the design of measurements, exposed deformation acting force type and orientation/direction, aspect ratio of workmaterial and acting force piece (indenter) \cite{002,031,034}. This in turn recalls the need for a fundamental hardness definition which is far to reach as the property itself is somehow an ad-hoc property comprised of subsequent effects of other interrelated structural properties like yield strength, tensile strength, modulus of elasticity, and etc \cite{003,034}. Keeping the inspirations from the practical experiments and measurements in mind, \textit{apparent} hardness can be realized as the mean contact pressure measured when the area of contact is at full load \cite{004}.

The \textit{apparent} hardness has been studied over three different size scales i.e. (i) macro, (ii) micro and (iii) nano \cite{005,001}. For instance, to rapidly retrieve mechanical properties in a bulk material with limited availability (small samples), the measurement of macro scale hardness can be considered. However, macro scale measurements are highly variable especially when the material might pose multi-phase or multi-crystalline structure with various grain sizes and orientations; reducing the reproducibility of results and violating the reliability and validation of findings \cite{006,035}. Indeed, considering the aspect ratio of acting indenter and the material as well as the magnitude of load (force) exerted on the material, the penetration depth into the material is too large that alters structural and mechanical properties itself upon acting. For such cases, then, the micro scale measurements need to be sought for, in where frequently loads up to 1kg are exerted onto the material surface while recording changes in applied load and penetration depth over time \cite{039}. The microstructure and different grains in the material can be captured in such measurements and as the depth/extend to which the indenter acts is too small, issues with macro scale measurements would not be encountered. Enhanced accuracy in the design and operation of spectroscopy devices brings the opportunity to measure and analysis the hardness more precisely in nano scales where loads of about 1 nano Newton are exerted onto the material while displacements as small as 1 nanometer per second can be operated which makes achievable the determination of maximum force tolerable by material \cite{041}. 

Despite the availability of high precision experimental devices, these measurements can be affected by operating/environmental parameters \cite{040} such as humidity which creates meniscus forces that, in the case of improper measurements and set-up management, makes findings unreliable \cite{007,008,009}. Indeed, the cost of material preparation and the time required to carry out measurements are drawbacks associated to hardness measurements \cite{037}. Thus, noting to the progress in computational methods and facilities, the theoretical investigations and simulation of material hardness in last decades have found much attention \cite{010} including (i) atomistic level modeling such as molecular dynamics \cite{011,012}, (ii) numerical continuum modeling such as finite element modeling and constituent equations \cite{013}, and (iii) coupled/combination of the atomistic and continuum models \cite{014} where key properties of material like younge modulus are initially predicted by molecular modeling techniques and then fed into continuum models \cite{015}. By current molecular modeling approaches, capturing hardness measurement process in full is not easy to achieve as it’s impossible to perform simulations utilizing real displacement steps used in spectroscopy practices i.e. 1 nm/s \cite{016,032} and this is one of the main reasons and motivations for development of coupled methods so that a long enough picture of process could be achievable even with lower accuracies \cite{015}. In such case, the effect of other factors including choice of indenter shape and geometry and the orientation of contact, which require repetitive simulations, can be attainable in a reasonable computational time. 

However, to simultaneously resolve such computational difficulties and retrieve quantum-mechanical levels of accuracy, the use of machine learning-based interatomic potentials (MLIPs) has found much interest over the last decade \cite{017,018,033}. These models correlate interatomic potential surface with atomistic level properties/descriptors so that quantum-mechanical (QM) accuracy will be retained within affordable computational costs \cite{019,020}. In such case, the reliability and versatility of correlations depend on how well the descriptors (atomic properties) have been chosen to reflect the \commentmak{[physicochemical]} characteristics of material as well as how well-established (complex) functionality has been considered \cite{021,022,023,024}. One of the most recent and efficient machine learning interatomic potentials are developed based on the moment tensor potentials (MTP) \cite{025}, by which most of quantum-mechanical models can be approximated by increasing the number of parameters in its basis functions in a systematic manner while keeping computational complexity scaling linearly with the number of atoms. 

The MLIPs require an initial fitting (training) into the ab initio data for correlating the coefficients in their basis functions, then they would be ready for utilization in property calculations \cite{026}. In simulations, it’s possible to encounter situation/configurations for which, the MLIPs might give over/underestimations which is an unavoidable characteristic of interpolative fitting/trainings \cite{027}. In order to update MLIPs for such extrapolations, then another fitting/training step is demanded referred as learning. Referring manual training of MLIPs as passive learning, then active learning is the case when MLIPs ask for ab initio data for the obtained extrapolations in the course of simulation from a databank (or an \textit{ab initio} calculation package) which oracles over the required information to the MLIPs \cite{028, 029}. Thus, active learning enables MLIPs to learn and train themselves on the fly \cite{030}. By treating extrapolations as local environments within an appropriate cutoff from the respective central atom, the retraining can be reduced/restricted to such environments only \cite{027,024}. Then, the cost of computations associated to the retraining of MLIPs can be reduced which is referred here as learning by neighborhood. 
